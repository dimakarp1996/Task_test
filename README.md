# Task_test
Результат выполнения тестового задания "обучить LSTM на стихах Пушкина" можно посмотреть в ноутбуке 'Пушкин(финальная версия).ipynb '. Процесс  выполнения задания можно посмотреть в ноутбуке 'Пушкин.ipynb'.

Что делал:

1) Обучал LSTM-модель на сборнике стихов Пушкина А.С, закодированном по принципу one-hot. Пробовал размеры окна 5 и 15.
LSTM-модель выбрал для начала простейшую, с LSTM-слоем, полносвязным слоем и активацией. Выбор архитектуры ограничивался вычислительными возможностями, т.к считать приходилось на CPU.

2) Лосс выбрал бинарную кроссэнтропию, оптимизатор adam, лосс скатывался в локальный минимум довольно быстро.

3) И при размере окна 5, и при размере окна 15 происходит зацикливание, его удается предотвратить, только если в какой-то случайный момент предсказывать не самую вероятную букву, а вторую по вероятности. Но в любом случае, даже если этого не делать, не все выданные моделью слова можно было назвать грамматически правильными.

Пример выдачи при 100 выходных ячейках LSTM и без dropout-слоя:

Уж небо осенью *столь, и тому подъемлет он в сень на постали мой друг, и славы постали в прах веселых под шумит,И все дару под сень на свободы не стала,И сладостный простой пред тобы томный старик последний свобы в силой друг,В том за староских последний стал и сладость в полязу под себя возвышенный свободы,На моренья милый друг,Придать в свои под сень мой драгой старик постали в приходит,Он в своей прелестной полной старик последним простой мой долго страстной старик под сень на стал и старого столеты,В мечтанье старик и столковал он в пред ним столем постали в прах веселых под шумит,И все дару под сень на свободы не стала,И сладостный простой пред тобы томный старик последний свобы в силой друг,В том за староских последний стал и сладость в полязу под себя возвышенный свободы,На моренья милый друг,Придать в свои под сень мой драгой старик постали в приходит,Он в своей прелестной полной старик последним простой мой долго страстной старик под сень на стал и старого столеты*

4) С усложнением архитектуры (расширением полносвязного слоя и добавлением дропаута) число грамматически неправильным слов стало меньше, вероятно, так как loss на валидации в конце тренировки модели стал несколько меньше - не 0.3, а 0.28 . Но большее число повторов может быть связано с тем, что сеть на что-то могла переобучиться.

Пример выдачи при 120 выходных ячейках LSTM и с dropout-слоем:

Уж небо осенью *долго в сердце простой не страшно в стране славы старины страны страны, друзья, приятной страны славы,Не стоят пред ним молодой, признаю страх на свете не смелой странный славы,Не достойный страны славы,На свет в страх не стал под сень моей друг, не стал под сень моей друг сердце волны своей свет и страшно в стране славы старины страны страны, другой приятный славы,Пред ним свободно в предан от своей сон он страх не стал под сень моей друг, не стал под сень моей друзья,Как славы славы славы, славы славы славы, славы славы не смелой,И все дружбе, под сень моей друг, славы бессмертный прихотит в нем сердце волны свед,То все друзья молодой,В сердце пред ним страх не стал последний свой долго в нем сестрица своих постели приветствовал он странный славы страны славы,Не подружками странный славы страны славы*

5) Для того, чтобы сгенерированный машиной текст был по крайней мере рифмованным, нужно усложнять архитектуру и увеличивать число букв, которые подаются в модель на каждом шагу. 15 букв - это меньше, чем целая строка. Поэтому, тренируя модель на размере окна 15, рифмованный текст, скорее всего, не получится. Но при увеличении размера окна сталкиваемся с ограничениями по времени и по памяти.

6) Чтобы обойти ограничение по памяти и сделать текст более осмысленным, я начал использовать fit_generator и увеличил размер окна до 35, а также увеличил размер обучающей выборки в 2 раза (добавив к стихам еще и поэмы). Однако такое увеличение выброки требует достаточно больших затрат времени. На текущий момент (середина дня 25 марта) модель обучается.

Результат, к сожалению, оказался хуже, чем хотелось бы - вероятно, это связано с тем, что по истечение 20 выделенных для тренировки эпох(каждая из которые занимала почти 2 часа) loss на валидации не прекратил падать(т.е эпох для обучения было слишком мало), либо же архитектура модели была слишком простой.

Пример выдачи:

Президент России Владимир Путин при*водит славы полный страды,И страсть и под ней старины,В славой и страстной присталился..."  скажи благословилась,Вольной старине слава поледали,И с нем волненье столодной простой,Под сенью полной страстью стариный старик,Мой старик сердце полести,На другом страстной полной,И страсть и старик отраданье,Как был он невольный страстной,В молчанье славость и старина,Ты пред нем столь полести подорожить молодой.Он голов под мой молодой,Он полны молодой не стал,И страстной не следов на старой,И славость и не приветал.Восторги старина своей прошли,И страсть и постали старины.Вот может быть он не полный,И страсть и не слез от стариной.Все только на свет отвечает:""Не старая страсть и почели,На страха своей полной,Поле волны сторожной страданье,Под молодой полести волна,И как сердце полести поледал,И страсть и не под угром и сладость отрадали.Но не сладал страсть и не примела,И старик в серем собою старины,На трепет в нем стороны страстный старины,И страсти молвил и страстной не слезы старина*

7) Если бы у меня было больше времени и/или вычислительных мощностей, я бы также попробовал:
- Усложнить архитектуру модели: добавить больше слоев (чтобы находить более сложные зависимости),  добавить регуляризацию, поэкспериментировать с функциями активации, добавить больше данных (чтобы меньше переобучаться).
- Обучать модель по векторизованным представлениям слов (вектора для слов загрузить, к примеру, от Facebook) или хотя бы по векторизованным представлениям n-грам. Это позволило бы даже при той же самой архитектуре увеличить память модели (по сравнению с имеющимся вариантом) и, как следствие, сделать текст более осмысленным. 
- Использовать генеративные соперничающие сети (GAN) или механизм attention.
