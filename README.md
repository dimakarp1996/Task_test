# Task_test
Обучал LSTM-модель на сборнике стихов Пушкина А.С, закодированном по принципу one-hot. Пробовал размеры окна 5 и 15.

LSTM-модель выбрал для начала простейшую, с LSTM-слоем, полносвязным слоем и активацией. Выбор архитектуры ограничивался вычислительными возможностями, т.к считать приходилось на CPU.

Лосс выбрал бинарную кроссэнтропию, оптимизатор adam, лосс скатывался в локальный минимум довольно быстро.

И при размере окна 5, и при размере окна 15 происходит зацикливание, его удается предотвратить, только если в какой-то случайный момент предсказывать не самую вероятную букву, а вторую по вероятности. Но в любом случае, даже если этого не делать, не все выданные моделью слова можно назвать грамматически правильными. С усложнением архитектуры(расширением полносвязного слоя и добавлением дропаута) число грамматически неправильным слов стало меньше.

Буду пробовать видоизменять архитектуру, если и этого не хватит - добавлять больше данных.

А для того, чтобы сгенерированный машиной текст был по крайней мере рифмованным, нужно увеличивать число букв, которые подаются в модель на каждом шагу. 15 букв - это меньше, чем целая строка(и поэтом , тренируя модель на рзамере окна 15, рифмованный текст ,скорее всего не получится). Но увеличение размера окна сталкивается с ограничениями по времени и по памяти.
Чтобы обойти ограничение по памяти и сделать текст более осмысленным, я начал использовать fit_generator и увеличил размер окнадо 35, но это требует достаточно больших затрат времени. На текущий момент модель обучается.
